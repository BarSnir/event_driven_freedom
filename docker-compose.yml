services:
  client-modern:
    container_name: modern-client
    build:
      context: ./client/modern/e_d_f
      dockerfile: Dockerfile
    volumes:
      - ".:/app"
      - "/app/node_modules"
    ports:
      - "5173:80"

  client-legacy:
    container_name: legacy-client
    build:
      context: ./client/legacy
      dockerfile: Dockerfile
    ports:
      - "5050:5050"
  db:
    container_name: legacy-mysql
    image: mysql:5.7
    platform: linux/x86_64
    restart: always
    attach: false
    command: --transaction-isolation=READ-COMMITTED --log-bin=binlog --binlog-format=ROW --server-id=1
    environment:
      MYSQL_DATABASE: "db"
      MYSQL_USER: "user"
      MYSQL_PASSWORD: "password"
      MYSQL_ROOT_PASSWORD: "password"
    ports:
      - "3306:3306"
    expose:
      - "3306"
    volumes:
      - my-db:/var/lib/mysql
  # zookeeper:
  #   image: confluentinc/cp-zookeeper:6.2.0
  #   container_name: zookeeper
  #   platform: linux/amd64
  #   restart: always
  #   attach: false
  #   healthcheck:
  #     test: echo 'ruok' | nc -w 2 localhost 2181 | grep imok
  #     timeout: 10s
  #     retries: 5
  #     interval: 30s
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #     KAFKA_OPTS: -Dzookeeper.4lw.commands.whitelist=ruok
  # broker:
  #   image: confluentinc/cp-kafka:6.2.0
  #   container_name: broker
  #   platform: linux/amd64
  #   attach: false
  #   restart: always
  #   healthcheck:
  #     test: nc -z localhost 9092 || exit -1
  #     start_period: 50s
  #     interval: 20s
  #     timeout: 10s
  #     retries: 10
  #   depends_on:
  #     zookeeper:
  #       condition: service_healthy
  #   ports:
  #     - 9092:9092
  #   environment:
  #     KAFKA_BROKER_ID: 1
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #     KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
  #     KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #     KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
  #     KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
  #     KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
  # schema-registry:
  #   image: confluentinc/cp-schema-registry:6.2.0
  #   container_name: schema-registry
  #   platform: linux/amd64
  #   attach: false
  #   restart: always
  #   depends_on:
  #     broker:
  #       condition: service_healthy
  #   healthcheck:
  #     test: curl --output /dev/null --silent --head --fail http://schema-registry:8082/subjects
  #     start_period: 5m
  #     interval: 30s
  #     timeout: 10s
  #     retries: 10
  #   ports:
  #     - "8082:8082"
  #   environment:
  #     SCHEMA_REGISTRY_HOST_NAME: schema-registry
  #     SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: broker:29092
  #     SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8082
  # kafka-connect:
  #   image: confluentinc/cp-kafka-connect-base:6.2.0
  #   container_name: kafka-connect
  #   platform: linux/amd64
  #   attach: false
  #   restart: always
  #   depends_on:
  #     schema-registry:
  #       condition: service_healthy
  #   ports:
  #     - 8083:8083
  #   healthcheck:
  #     test: curl http://kafka-connect:8083/ || exit 1
  #     start_period: 3m
  #     interval: 30s
  #     timeout: 10s
  #     retries: 1
  #   environment:
  #     CONNECT_BOOTSTRAP_SERVERS: "broker:29092"
  #     CONNECT_REST_PORT: 8083
  #     CONNECT_GROUP_ID: kafka-connect
  #     CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
  #     CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
  #     CONNECT_STATUS_STORAGE_TOPIC: _connect-status
  #     CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
  #     CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
  #     CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8082'
  #     CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
  #     CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
  #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
  #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
  #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
  #   command: >
  #     bash -c " echo Installing Debezium && \
  #     confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.7.0 && \
  #     echo 'Launching Kafka Connect worker' && \
  #     /etc/confluent/docker/run"
  # control-center-a:
  #   image: confluentinc/cp-enterprise-control-center:6.2.0
  #   container_name: control-center-a
  #   platform: linux/amd64
  #   restart: always
  #   attach: false
  #   depends_on:
  #     kafka-connect:
  #       condition: service_healthy
  #   healthcheck:
  #     test: curl http://localhost:9021/ || exit 1
  #     start_period: 3m
  #     interval: 30s
  #     timeout: 10s
  #     retries: 1
  #   ports:
  #     - "9021:9021"
  #   environment:
  #     CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'
  #     CONTROL_CENTER_CONNECT_CONNECT_CLUSTER: 'kafka-connect:8083'
  #     CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8082"
  #     CONFLUENT_METRICS_TOPIC_REPLICATION: 1
  #     CONTROL_CENTER_REPLICATION_FACTOR: 1
  #     CONTROL_CENTER_COMMAND_TOPIC_REPLICATION: 1
  #     CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_REPLICATION: 1
  #     CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
  #     CONTROL_CENTER_INTERNAL_TOPICS_REPLICATION: 1
  #     CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
  #     CONTROL_CENTER_STREAMS_NUM_STREAM_THREADS: 1
  #     CONTROL_CENTER_STREAMS_CACHE_MAX_BYTES_BUFFERING: 104857600
  # command:
  #   - bash
  #   - -c
  #   - |
  #     /etc/confluent/docker/run
  jobmanager:
    container_name: jobmanager
    platform: linux/arm64/v8
    build:
      context: ./images/pyflink
      dockerfile: .Dockerfile
    ports:
      - "8081:8081"
    command: jobmanager
    attach: false
    restart: always
    volumes:
      - ./datasets:/opt/flink/datasets
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        jobmanager.memory.process.size: 2000m
  taskmanager:
    container_name: taskmanager
    platform: linux/arm64/v8
    attach: false
    restart: always
    build:
      context: ./images/pyflink
      dockerfile: .Dockerfile
    depends_on:
      - jobmanager
    command: taskmanager
    volumes:
      - ./datasets:/opt/flink/datasets
    scale: 1
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 8
        parallelism.default: 8
        taskmanager.memory.process.size: 7500m
        taskmanager.memory.framework.off-heap.size: 500m
        taskmanager.memory.task.off-heap.size: 1000m
        taskmanager.memory.jvm-metaspace.size: 600m
    links:
      - db:db
  pyflink-client:
    platform: linux/arm64/v8
    container_name: pyflink-client
    build:
      context: ./images/pyflink
      dockerfile: .Dockerfile
    depends_on:
      - taskmanager
    command: >
      bash -c  "chmod +x ./project && chmod +x ./project/scripts/steps.sh && \
      ./project/scripts/steps.sh"
    environment:
      - DATABASE=production
      - DB_HOST=db
      - DB_USER=root
      - DB_PASSWORD=password
      - ENV=development
      - DB_CONFIG=/opt/flink/project/configs/database.json
      - SCRIPTS_PATH_DIR=/opt/flink/project/scripts
    volumes:
      - ./flink_process:/opt/flink/ops
      - ./jars:/opt/flink/opt
      - ./datasets:/opt/flink/datasets
      - .:/opt/flink/project
    scale: 1
volumes:
  my-db:
